{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "* ### nn.Module\n",
    "  最基本的create model方式,彈性最大,需要override __init__() , __forward__()。<br>\n",
    "  缺點: 如果layer很多,寫forward很麻煩,通常是搭配Sequential和ModuleList寫好的block使用。\n",
    "* ### nn.Sequential\n",
    "  如果要建構的模型很單純,不需要定義特殊的forward,使用Sequential。(Sequential是Module的subclass,已經內建forward) <br>\n",
    "  注意1:因為nn.Sequential已經內建forward,所以必須確定上下兩層layer之間的input和output是否吻合。<br>\n",
    "  注意2:可以直接使用nn.Sequential來定義整個model,但是彈性差(無法自己定義forward),通常是用nn.Sequential來定義block並和nn.Module一起使用。<br>\n",
    "  Ex. (conv2d + Relu) * 2的架構為例 \n",
    "  * 方法一:直接將torch.nn中的元件當參數傳入 <br>\n",
    "    缺點:layer name無法指定,會用預設的數字。\n",
    "  * 方法二:用OrderedDict當參數傳入。<br>\n",
    "    優點：可指定layer name <br>\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1(\n",
      "  (seq): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#方法一\n",
    "class net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net1, self).__init__()\n",
    "        self.seq = nn.Sequential(nn.Conv2d(1, 20, 3, 1), nn.ReLU(), nn.Conv2d(20, 20, 3, 1), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "    #def forward(self, x):\n",
    "    #    for s in self.seq:\n",
    "    #        x = s(x)\n",
    "    #    return x\n",
    "net = net1()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net2(\n",
      "  (seq): Sequential(\n",
      "    (conv1): Conv2d(1, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (relu1): ReLU()\n",
      "    (conv2): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#方法二\n",
    "from collections import OrderedDict\n",
    "class net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net2, self).__init__()\n",
    "        self.seq = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(1, 20, 3, 1)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv2d(20, 20 ,3,1)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "    def forward(self, x):\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "    #def forward(self, x):\n",
    "    #    for s in self.seq:\n",
    "    #        x = s(x)\n",
    "    #    return x\n",
    "net = net2()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### nn.ModuleList\n",
    "  nn.ModuleList是一個存放不同module的容器,並會把module的parameter(weight, bias)存到network中。<br>\n",
    "  注意1:ModuleList沒有內建forward所以,存在ModuleList內的module的順序沒有任何太大意義。<br>\n",
    "  注意2:操作方式和python的list類似,有extend和append,但是一般python的list並不會將module的parameter(weight, bias)存到network中。<br>\n",
    "  使用時機1:如果某個block或model重複性的東西太多,可用modulelist來完成(透過for迴圈)，但可用nn.Sequential實現。<br>\n",
    "  使用時機2:如果像是ResNet的short cut或是FCN中的skip architecture需要和前面層做concate或者add的,使用modulelist較佳。\n",
    "  \n",
    "* ### 補充:torch.nn和torch.nn.functional比較 \n",
    "  [torch.nn和torch.nn.functional]( https://www.zhihu.com/question/66782101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net3(\n",
      "  (list): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "net4(\n",
      "  (seq): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (4): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (seq2): Sequential(\n",
      "    (linear_0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (linear_1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (linear_2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (linear_3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (linear_4): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "net5(\n",
      "  (cnn_list): ModuleList(\n",
      "    (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(20, 30, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "torch.Size([100, 20, 8, 8])\n",
      "torch.Size([100, 20, 6, 6])\n",
      "torch.Size([100, 30, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "#使用時機1 範例(nn.ModuleList Version)\n",
    "class net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net3, self).__init__()\n",
    "        self.list = nn.ModuleList([nn.Linear(10, 10) for i in range(5)])\n",
    "    def forward(self, x):\n",
    "        for layer in self.list:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "net = net3()\n",
    "print(net)  \n",
    "\n",
    "#使用時機1 範例(nn.Sequential Version)\n",
    "class net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net4, self).__init__()\n",
    "        self.list = [nn.Linear(10, 10) for i in range(5)]\n",
    "        \n",
    "        # 重要!!! : *這個符號可以將list拆開成一個一個element並按造順序\n",
    "        self.seq = nn.Sequential(*self.list)\n",
    "        self.seq2 = nn.Sequential()\n",
    "        for l in self.list:\n",
    "            self.seq2.add_module('linear_'+ str(self.list.index(l)), l)\n",
    "    def forward(self, x):\n",
    "        x = seq(x)\n",
    "        x = seq2(x)\n",
    "        return x\n",
    "    \n",
    "net = net4()\n",
    "print(net)  \n",
    "\n",
    "#使用時機2 範例\n",
    "def conv_block(nums_layer):\n",
    "    block =[]\n",
    "    for i in range(nums_layer):\n",
    "        if (i+1) % 3 == 0:\n",
    "            block.append(nn.Conv2d(20,30,3,1))\n",
    "        else:\n",
    "            block.append(nn.Conv2d(20,20,3,1))\n",
    "    return block \n",
    "class net5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net5, self).__init__()\n",
    "        self.cnn_list = nn.ModuleList(conv_block(3))\n",
    "        self.trace = []\n",
    "    def forward(self, x):\n",
    "        for layer in self.cnn_list:\n",
    "            x = layer(x)\n",
    "            self.trace.append(x)\n",
    "        return x\n",
    "net = net5()\n",
    "print(net)\n",
    "input_tensor  = torch.randn(100, 20, 10, 10) # input batch size: 100 , channel:20, h:10, w:10\n",
    "output = net(input_tensor)\n",
    "for each in net.trace:\n",
    "    print(each.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG 16 & VGG 19\n",
    "\n",
    "cfg: 定義Feature extraction layer架構 <br>\n",
    "\n",
    "#### [def]<br>\n",
    "vgg16:對外的api接口<br>\n",
    "_vgg: create model 並且載入 pretrain weight<br>\n",
    "make_layers: create model<br>\n",
    "\n",
    "#### [class] <br>\n",
    "vgg: 將Feature extraction layer和classifier串連起來，並定義整個forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_urls = {\n",
    "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
    "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
    "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
    "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
    "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
    "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
    "}\n",
    "cfg = {\n",
    "      'vgg16':[64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "      'vgg19':[64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M']\n",
    "}\n",
    "class vgg(nn.Module):\n",
    "    def __init__(self, feature, num_classes=1000, init_weights=True):\n",
    "        super(vgg, self).__init__()\n",
    "        self.feature = feature\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(nn.Linear(512 * 7 * 7, 4096),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Dropout(0.25),\n",
    "                                        nn.Linear(4096, 4096),\n",
    "                                        nn.ReLU(inplace=True),\n",
    "                                        nn.Dropout(0.25),\n",
    "                                        nn.Linear(4096, num_classes)\n",
    "                                       )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        #將feature map的輸出整理成 7*7(因為input可能是不同size的image)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layers(arch, batch_norm):\n",
    "    layers = nn.Sequential()\n",
    "    in_channel = 3\n",
    "    conv_idx = 0\n",
    "    bn_idx = 0\n",
    "    pool_idx = 0\n",
    "    relu_idx = 0\n",
    "    #使用.add_module來串接model是為了對每一個layer命名，如果不想命名可以直接將每一個layer塞到一個list\n",
    "    #並使用nn.Sequential(*list)快速串連模型\n",
    "    for l in cfg[arch[:5]]:\n",
    "        if l == 'M':\n",
    "            layers.add_module('max_pooling_' + str(pool_idx), nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "            pool_idx += 1\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channel, l, 3, 1, padding=1)\n",
    "            layers.add_module('conv2d_' + str(conv_idx), conv2d)\n",
    "            conv_idx += 1\n",
    "            if batch_norm:\n",
    "                layers.add_module('batch_norm_' + str(bn_idx), nn.BatchNorm2d(l))\n",
    "                layers.add_module('relu_' + str(relu_idx), nn.ReLU(inplace=True))\n",
    "                bn_idx += 1\n",
    "                relu_idx += 1\n",
    "            else:\n",
    "                layers.add_module('relu_' + str(relu_idx), nn.ReLU(inplace=True))\n",
    "                relu_idx += 1\n",
    "            in_channel = l\n",
    "    return layers\n",
    "\n",
    "from torch.hub import load_state_dict_from_url\n",
    "def _vgg(arch, pretrain, batch_norm, **kwargs):\n",
    "    if pretrain:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = vgg(make_layers(arch, batch_norm), **kwargs)\n",
    "    if pretrain:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch])\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "def vgg16Bn(pretrain=False, **kwargs):\n",
    "    return _vgg('vgg16_bn', pretrain, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg(\n",
      "  (feature): Sequential(\n",
      "    (conv2d_0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_0): ReLU(inplace=True)\n",
      "    (conv2d_1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_1): ReLU(inplace=True)\n",
      "    (max_pooling_0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2d_2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (conv2d_3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_3): ReLU(inplace=True)\n",
      "    (max_pooling_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2d_4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_4): ReLU(inplace=True)\n",
      "    (conv2d_5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_5): ReLU(inplace=True)\n",
      "    (conv2d_6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_6): ReLU(inplace=True)\n",
      "    (max_pooling_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2d_7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_7): ReLU(inplace=True)\n",
      "    (conv2d_8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_8): ReLU(inplace=True)\n",
      "    (conv2d_9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_9): ReLU(inplace=True)\n",
      "    (max_pooling_3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2d_10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_10): ReLU(inplace=True)\n",
      "    (conv2d_11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_11): ReLU(inplace=True)\n",
      "    (conv2d_12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (batch_norm_12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu_12): ReLU(inplace=True)\n",
      "    (max_pooling_4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.25, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.25, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=14, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg16Bn = vgg(make_layers('vgg16_bn', batch_norm=True), 14, False)\n",
    "print(vgg16Bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "train_trm = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.4631, 0.4959, 0.4289],std=[0.2755, 0.2713, 0.3078])\n",
    "            ])\n",
    "val_trm = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.4631, 0.4959, 0.4289],std=[0.2755, 0.2713, 0.3078])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1815"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ImageFolder\n",
    "import torchvision.datasets as dset\n",
    "train_root_dir = '/Users/lionl771128/Documents/AI_Projdct/dataset1122/train/image/tree'\n",
    "val_root_dir = '/Users/lionl771128/Documents/AI_Projdct/dataset1122/test/image/tree'\n",
    "train_set_image_folder = dset.ImageFolder(root=train_root_dir, transform=train_trm)\n",
    "val_set_image_folder = dset.ImageFolder(root=val_root_dir, target_transform=val_trm)\n",
    "len(train_set_image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset=train_set_image_folder,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4\n",
    "                         )\n",
    "\n",
    "test_loader = DataLoader(dataset=val_set_image_folder,\n",
    "                        batch_size=16,\n",
    "                        shuffle=True,\n",
    "                        num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "計算mean以及std，供transforms.Normalize使用\n",
    "'''\n",
    "import numpy as np\n",
    "def get_mean_std(dataset, batch_size):\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    #train = iter(train_loader).next()[0]\n",
    "    s = torch.Tensor()\n",
    "    for data, target in train_loader:\n",
    "        s = torch.cat((s, data), 0)\n",
    "      \n",
    "    mean = torch.mean(s, (0, 2, 3))\n",
    "    std = torch.std(s, (0, 2, 3))\n",
    "    print(mean, std)\n",
    "\n",
    "# train_set_ = leaf_tree(root_dict='C:/Users/scott/AI_Project/dataset1122',\n",
    "#                       sub_dict ='tree',\n",
    "#                       transforms=transforms.Compose([transforms.Resize((446, 446)),\n",
    "#                                 transforms.ToTensor()])\n",
    "#                      )\n",
    "# get_mean_std(train_set_, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from apex import amp\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”\n",
    "# with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#     scaled_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm \n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = vgg(make_layers(arch='vgg16_bn', batch_norm=True), 14, True).to(device)\n",
    "optimizer = optm.Adam(model.parameters())\n",
    "schedule = StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "nums_epoch = 100\n",
    "\n",
    "# from apex import amp\n",
    "# model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\") # 这里是“欧一”，不是“零一”\n",
    "for epoch in range(nums_epoch):\n",
    "    idx = 0\n",
    "    model.train()\n",
    "    with tqdm(train_loader, leave=True, total=len(train_loader)) as t:\n",
    "        t.set_description('Epoch %d/%d' %(epoch+1, nums_epoch))\n",
    "        for data, target in t:\n",
    "            optimizer.zero_grad()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            print(output.shape)\n",
    "            print(output[0])\n",
    "            print(output.argmax(dim=1, keepdim=True))\n",
    "            p = output.argmax(dim=1, keepdim=True)\n",
    "            print(p.eq(target).sum().item())\n",
    "            print(p.eq(target))\n",
    "            print(target)\n",
    "            print(len(train_loader.dataset))\n",
    "            print(len(train_loader))\n",
    "            loss = F.cross_entropy(output, target)\n",
    "#             with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "#                 scaled_loss.backward()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            idx += 1\n",
    "            if idx % len(train_loader) != 0:\n",
    "                t.set_postfix(train_loss=loss.item())\n",
    "            else:\n",
    "                model.eval()\n",
    "                correct = 0\n",
    "                val_loss = 0\n",
    "                for data, target in val_loader:\n",
    "                    with torch.no_grad():\n",
    "                        data, target = data.to(device), target.to(device)\n",
    "                        output = model(data)\n",
    "                        val_loss += F.cross_entropy(output, target)\n",
    "                        pred = output.argmax(dim=1)\n",
    "                        correct += pred.eq(target).sum().item()\n",
    "                        \n",
    "                accuracy = 100. * correct / len(val_loader.dataset)\n",
    "                t.set_postfix(train_loss=loss.item(),val_loss=val_loss.item(), accuracy=accuracy)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import os, io\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "class leaf_tree(Dataset):\n",
    "    def __init__(self, root_dict, sub_dict, transforms, train=True):\n",
    "        self.transforms = transforms\n",
    "        dir_type = 'train' if train else 'test'\n",
    "        paths = []\n",
    "        labels = []\n",
    "        label = 0\n",
    "        folders = glob(root_dict + '/' + dir_type + '/image/' + sub_dict + '/*')\n",
    "        for f in folders:\n",
    "            p = glob(f + '/*.[jJ][pP][gG]')\n",
    "            paths.extend(p)\n",
    "            labels.extend([label] * len(p))\n",
    "            label += 1\n",
    "        image_df = pd.DataFrame({\n",
    "            'path':paths,\n",
    "            'label':labels\n",
    "        })\n",
    "        self.image_df = image_df\n",
    "    def __len__(self):\n",
    "        return len(self.image_df)\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_df.iloc[idx, 0]\n",
    "        label = self.image_df.iloc[idx, 1]\n",
    "        image = Image.open(path)\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        return image, label\n",
    "\n",
    "#'https://medium.com/@rowantseng/pytorch-%E8%87%AA%E5%AE%9A%E7%BE%A9%E8%B3%87%E6%96%99%E9%9B%86-custom-dataset-7f9958a8ff15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = leaf_tree(root_dict='C:/Users/scott/AI_Project/dataset1122',\n",
    "                      sub_dict ='tree',\n",
    "                      transforms=train_trm\n",
    "                     )\n",
    "val_set = leaf_tree(root_dict='C:/Users/scott/AI_Project/dataset1122',\n",
    "                    sub_dict='tree',\n",
    "                    transforms=val_trm,\n",
    "                    train=False\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=16,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0\n",
    "                         )\n",
    "val_loader = DataLoader(dataset=val_set,\n",
    "                        batch_size=16,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
